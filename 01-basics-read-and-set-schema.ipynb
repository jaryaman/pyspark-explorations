{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18332549-f39d-48f1-99a7-553c1920fc96",
   "metadata": {},
   "source": [
    "# Reading and mutating spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69b7364-d27c-4ed7-b86d-10f959486362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession  # we import from SQL so we can make direct SQL queries\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6768c26-e8fc-4303-8e16-84384f51ca1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/08 16:16:44 WARN Utils: Your hostname, GBLON1WLZ13699 resolves to a loopback address: 127.0.1.1; using 10.164.5.249 instead (on interface eth2)\n",
      "21/07/08 16:16:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/juvid/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/07/08 16:16:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb49d1b-3dc3-4e12-8c9e-8c19e903a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../pyspark/Python-and-Spark-for-Big-Data-master/Spark_DataFrames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab6b66d-00a9-47c2-ba9e-a3734e540d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = str(data_dir/'people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94ca3b3f-b6d5-42e0-8f8e-43aef8900b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "154cd2b6-b3d1-4ca5-973b-afe7446fe9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b77c4e4f-1a73-4c1e-8d44-a81af12ea0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09a2c46-5d33-41d7-91d9-eb8af7352ac0",
   "metadata": {},
   "source": [
    "Spark can do schema inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8757f517-d0e7-4f58-b92e-2ab55a5acf1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be78f26-686f-4625-bd3b-bf511bfcdaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|summary|               age|   name|\n",
      "+-------+------------------+-------+\n",
      "|  count|                 2|      3|\n",
      "|   mean|              24.5|   null|\n",
      "| stddev|7.7781745930520225|   null|\n",
      "|    min|                19|   Andy|\n",
      "|    max|                30|Michael|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3a1c6c-3a58-49a3-bcd9-e57459185f1a",
   "metadata": {},
   "source": [
    "## Schema specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a785fa-2e01-47dc-910c-c0279b7d5ea3",
   "metadata": {},
   "source": [
    "Sometimes, for complicated data, the schema inference won't work and we need to manually specify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17ef3983-6aba-44e1-b10c-347efb9a2af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d16deab3-b2d9-43dc-ba65-9a6b81acda0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField('age', IntegerType(), True),  # the third argument is if the quantity can be null\n",
    "               StructField('name', StringType(), True),\n",
    "              ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24bffaeb-5b6b-44a9-9c12-170d131a2bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_struct = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "592f9938-2ba0-46b1-8c70-de434b8668c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(data_path, schema=final_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bbcd1ab-5eb4-4982-965d-2feed35b345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e42995a-e613-4782-a155-e47728603d65",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee10231a-a78f-4488-afc6-d00fbb8cf178",
   "metadata": {},
   "source": [
    "## Selecting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce1ab8-d7fe-4d24-9376-bf02c0e3a419",
   "metadata": {},
   "source": [
    "Just getting columns gives a column object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9a59670-f035-4e09-91ec-e4b7ff3d4225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c013f33-1f3a-4ca1-9aec-4c5ab74b4d89",
   "metadata": {},
   "source": [
    "But using `select` to get a dataframe object is often more flexible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d56899e-d322-4da3-b305-0f17975d8d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a63bd9f6-89fc-42f9-ac3b-cdd138a1936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8a28fbe-7393-421e-8c1c-cb992100e497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['age', 'name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed77992-6ad7-44a6-8b3c-b16f3aaac52e",
   "metadata": {},
   "source": [
    "## Mutating dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481fa78-8bf1-49d2-90d2-ecd5574747a1",
   "metadata": {},
   "source": [
    "Adding a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1a4a18b-8b1c-462f-a17b-56492eedcfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------+\n",
      "| age|   name|double_age|\n",
      "+----+-------+----------+\n",
      "|null|Michael|      null|\n",
      "|  30|   Andy|        60|\n",
      "|  19| Justin|        38|\n",
      "+----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('double_age', df['age']*2).show()  # needs a column object here, not a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3222f45-d367-40d5-b13e-fe0c76b75b30",
   "metadata": {},
   "source": [
    "But this is not an inplace operation, would need to save to a new dataframe to retain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af656c09-4de4-447c-b24c-4cb34d3d2dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c9b09-9e42-4139-bc0d-6e55f9242427",
   "metadata": {},
   "source": [
    "Renaming a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3752928-cd6c-41fc-97dc-56932f378f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|my_new_age|   name|\n",
      "+----------+-------+\n",
      "|      null|Michael|\n",
      "|        30|   Andy|\n",
      "|        19| Justin|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('age', 'my_new_age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606ea10-6c07-4964-9c8d-375c28e87e01",
   "metadata": {},
   "source": [
    "## Using SQL to interact with a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3300c57-06a8-477e-a226-56dd98e11885",
   "metadata": {},
   "source": [
    "Register the dataframe as a SQL temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d60bcd4b-1efb-4909-b8de-2d70e1dbaca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('people')  # table name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3fa8f60e-5836-4b8e-9a58-6d47c1224f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.sql(\"SELECT * FROM people WHERE age > 20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41dd194c-3bd1-49f6-8349-5cf097eab81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56826078-3475-4566-9e64-8f41a591d8c4",
   "metadata": {},
   "source": [
    "So we can use SQL syntax with spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
