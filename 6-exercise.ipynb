{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c399150b-04f8-47bd-aa6d-2b9e8847beed",
   "metadata": {},
   "source": [
    "# Exercise on pyspark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d542662-e243-46f9-80d5-db171cff1073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "data_dir = Path('/home/juvid/Python-and-Spark-for-Big-Data-master/Spark_DataFrame_Project_Exercise/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54681e44-b34d-484c-bb5d-896258d9bbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/07/28 09:36:39 WARN Utils: Your hostname, GBLON1WLZ13699 resolves to a loopback address: 127.0.1.1; using 10.164.16.79 instead (on interface eth2)\n",
      "21/07/28 09:36:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/juvid/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/07/28 09:36:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('ex').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b32da1-7638-47fa-866a-3d3d574b3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(str(data_dir/'walmart_stock.csv'), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3298cb36-ebb5-4c42-a29c-d0d40f0d0b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc116cf-1e69-442d-9767-c89cd023390e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c165ae7b-b172-49cc-97ab-795f136bef12",
   "metadata": {},
   "source": [
    "This isn't the right schema, let's try and set it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d4f4d91-f303-4da4-bb8d-707f8b5260cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, DateType, DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86584051-fbb1-48cb-9dbc-1055118f596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = [StructField('Date', DateType(), True),  \n",
    "               StructField('Open', DoubleType(), True),\n",
    "               StructField('High', DoubleType(), True),\n",
    "               StructField('Low', DoubleType(), True),\n",
    "               StructField('Close', DoubleType(), True),\n",
    "               StructField('Volume', IntegerType(), True),\n",
    "               StructField('Adj Close', DoubleType(), True),\n",
    "              ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "797fde88-765c-4554-abfd-bbc4f1633bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(str(data_dir/'walmart_stock.csv'), schema = StructType(fields=data_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "945ebbd9-e860-4a90-a7a3-b3eead3aa8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e468cd80-e78c-4582-a90a-a8e2ae696e51",
   "metadata": {},
   "source": [
    "Print first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f5b33cc-14d6-4e5b-9073-16cad3525bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=None, Open=None, High=None, Low=None, Close=None, Volume=None, Adj Close=None),\n",
       " Row(Date=datetime.date(2012, 1, 3), Open=59.970001, High=61.060001, Low=59.869999, Close=60.330002, Volume=12668800, Adj Close=52.619234999999996),\n",
       " Row(Date=datetime.date(2012, 1, 4), Open=60.209998999999996, High=60.349998, Low=59.470001, Close=59.709998999999996, Volume=9593300, Adj Close=52.078475),\n",
       " Row(Date=datetime.date(2012, 1, 5), Open=59.349998, High=59.619999, Low=58.369999, Close=59.419998, Volume=12768200, Adj Close=51.825539),\n",
       " Row(Date=datetime.date(2012, 1, 6), Open=59.419998, High=59.450001, Low=58.869999, Close=59.0, Volume=8069400, Adj Close=51.45922)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27246182-6c26-4796-8893-0a8a0cf7d625",
   "metadata": {},
   "source": [
    "Describe the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f7c5815-7309-4a34-8fe9-a63cd329f7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
      "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
      "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
      "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
      "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd30587-99f8-4724-a5c4-59dc35ea9fcf",
   "metadata": {},
   "source": [
    "Format to 2 dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cca97bc-99e7-4a37-a41e-a479ed20d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4542098-d17f-4ce8-8fef-6b9238087dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- summary: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "description.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e81c67ba-ff23-4d62-86d5-22bc39f0e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90138cda-e83c-450f-95e2-ff72d4cad0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+--------+-------------+---------+\n",
      "|summary|    Open|    High|   Close|       Volume|Adj Close|\n",
      "+-------+--------+--------+--------+-------------+---------+\n",
      "|  count|1,258.00|1,258.00|1,258.00|     1,258.00| 1,258.00|\n",
      "|   mean|   72.36|   72.84|   72.39| 8,222,093.50|    67.24|\n",
      "| stddev|    6.77|    6.77|    6.76| 4,519,781.00|     6.72|\n",
      "|    min|   56.39|   57.06|   56.42| 2,094,900.00|    50.36|\n",
      "|    max|   90.80|   90.97|   90.47|80,898,096.00|    84.91|\n",
      "+-------+--------+--------+--------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "description.select(description['summary'],\n",
    "                   format_number(description['Open'].cast('float'), 2).alias('Open'),\n",
    "                   format_number(description['High'].cast('float'), 2).alias('High'),\n",
    "                   format_number(description['Close'].cast('float'), 2).alias('Close'),\n",
    "                   format_number(description['Volume'].cast('float'), 2).alias('Volume'),\n",
    "                   format_number(description['Adj Close'].cast('float'), 2).alias('Adj Close'),\n",
    "                  ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1555ee5-7307-4ec5-af24-259c88c9d772",
   "metadata": {},
   "source": [
    "Create new datafrane with a new column HV ratio = High price / volume per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10365555-4bf8-4988-a244-f9dc059822da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_hv = df.withColumn('HV Ratio', df['High']/df['Volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50db2639-0860-4f84-8105-dcf064d865b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            HV ratio|\n",
      "+--------------------+\n",
      "|                null|\n",
      "|4.819714653321546E-6|\n",
      "|6.290848613094555E-6|\n",
      "|4.669412994783916E-6|\n",
      "|7.367338463826307E-6|\n",
      "|8.915604778943901E-6|\n",
      "|8.644477436914568E-6|\n",
      "|9.351828421515645E-6|\n",
      "| 8.29141562102703E-6|\n",
      "|7.712212102001476E-6|\n",
      "|7.071764823529412E-6|\n",
      "|1.015495466386981E-5|\n",
      "|6.576354146362592...|\n",
      "| 5.90145296180676E-6|\n",
      "|8.547679455011844E-6|\n",
      "|8.420709512685392E-6|\n",
      "|1.041448341728929...|\n",
      "|8.316075414862431E-6|\n",
      "|9.721183814992126E-6|\n",
      "|8.029436027707578E-6|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_hv.select('HV ratio').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717947d3-367f-4596-85ab-2fb358f6b1ed",
   "metadata": {},
   "source": [
    "Find the day with the largest high price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "154eed3e-7c1d-4c74-a37a-2de60109ccc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2015, 1, 13)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.orderBy('High', ascending=False).head(1)[0]['Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf0b62-a500-4120-a1a7-1f683ed906b0",
   "metadata": {},
   "source": [
    "Find the mean close price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0afec9a1-5a3f-4502-b5a1-3ad9772d5676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Close)|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Close': 'avg'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4916fcd-ed09-44b9-beaa-9e98a764d88b",
   "metadata": {},
   "source": [
    "Find max and min volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "788a68fb-5dcf-482f-bd53-38b5abaeaedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max as py_max\n",
    "from pyspark.sql.functions import min as py_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94363857-85b1-4388-8844-4ca125e1cb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|max(Volume)|min(Volume)|\n",
      "+-----------+-----------+\n",
      "|   80898100|    2094900|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(py_max('Volume'), py_min('Volume')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3728253f-5e24-4d9e-9405-e9b0f0a0573a",
   "metadata": {},
   "source": [
    "Number of days close was lower than 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "43ebe3d2-7073-4ae4-acfd-b61903abcbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['Close'] < 60).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ed8574-3a72-4d61-8fdc-1d57dd746e65",
   "metadata": {},
   "source": [
    "Percentage of time High was greater than 80 dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "37419925-7b95-42e0-8152-405012037d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.134233518665608"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['High'] > 80).count()/df.count()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f364279b-acc5-4443-b33f-ad531add9aec",
   "metadata": {},
   "source": [
    "Correlation between High and Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b2690f0c-289b-436b-b762-ea3316e5fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "43e79fe5-68ef-4ef7-a0f7-1da1688c3637",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop()  # the following method is sensitive to NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2dc225e5-c824-49a5-8191-765208f9b34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3384326061737161"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.corr('High', 'Volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36468c94-b81f-4731-b580-e2a997447989",
   "metadata": {},
   "source": [
    "Max high per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5f41e62a-64ff-47b9-bc68-38280bcef0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "049c004e-1f2a-4b34-b3b8-18d4ecdd80aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 85:=========================================>            (154 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Year|max(High)|\n",
      "+----+---------+\n",
      "|2012|77.599998|\n",
      "|2013|81.370003|\n",
      "|2014|88.089996|\n",
      "|2015|90.970001|\n",
      "|2016|75.190002|\n",
      "+----+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tmp = df.withColumn('Year', year('Date'))\n",
    "tmp.groupby('Year').agg({'High': 'max'}).sort('Year').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa4cb5a-2861-4c31-80ed-0052de964d69",
   "metadata": {},
   "source": [
    "Average close for each calendar month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "acf06ecc-1f3c-406e-a105-cfecfffe7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0662d95c-29a3-471b-a852-6dc0bdd99579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 87:============================================>         (164 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Month|       avg(Close)|\n",
      "+-----+-----------------+\n",
      "|    1|71.44801958415842|\n",
      "|    2|  71.306804443299|\n",
      "|    3|71.77794377570092|\n",
      "|    4|72.97361900952382|\n",
      "|    5|72.30971688679247|\n",
      "|    6| 72.4953774245283|\n",
      "|    7|74.43971943925233|\n",
      "|    8|73.02981855454546|\n",
      "|    9|72.18411785294116|\n",
      "|   10|71.57854545454543|\n",
      "|   11| 72.1110893069307|\n",
      "|   12|72.84792478301885|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tmp = df.withColumn('Month', month('Date'))\n",
    "tmp.groupby('Month').agg({'Close': 'avg'}).sort('Month').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd0bc00-c92b-4792-862b-24d921ac7a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
